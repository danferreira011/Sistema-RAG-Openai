import streamlit as st
import os
from pathlib import Path
from dotenv import load_dotenv
import tempfile
import shutil

# Importa√ß√µes para processamento de documentos
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from openai import OpenAI

# Carregar vari√°veis de ambiente
load_dotenv()

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Sistema RAG Empresarial",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1e3a8a;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton>button {
        background-color: #1e3a8a;
        color: white;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #2563eb;
    }
    .info-box {
        background-color: #f0f9ff;
        border-left: 5px solid #3b82f6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 5px;
    }
    </style>
""", unsafe_allow_html=True)

class RAGSystem:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.api_key) if self.api_key else None
        self.vectorstore_path = "vectorstore"
        self.documents_path = "documentos"
        self.vectorstore = None
        self.qa_chain = None
        self.memory = None
        
        # Criar diret√≥rios se n√£o existirem
        Path(self.vectorstore_path).mkdir(exist_ok=True)
        Path(self.documents_path).mkdir(exist_ok=True)
        
    def validate_api_key(self):
        """Valida se a API Key est√° configurada"""
        if not self.api_key:
            st.error("‚ö†Ô∏è API Key da OpenAI n√£o encontrada! Configure no arquivo .env")
            st.info("Adicione OPENAI_API_KEY=sua-chave no arquivo .env")
            return False
        return True
    
    def load_documents(self, uploaded_files):
        """Carrega e processa documentos PDF"""
        documents = []
        
        for uploaded_file in uploaded_files:
            # Salvar arquivo temporariamente
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            # Carregar PDF
            loader = PyPDFLoader(tmp_file_path)
            docs = loader.load()
            
            # Adicionar metadados
            for doc in docs:
                doc.metadata['source'] = uploaded_file.name
                doc.metadata['file_type'] = 'pdf'
            
            documents.extend(docs)
            
            # Limpar arquivo tempor√°rio
            os.unlink(tmp_file_path)
            
        return documents
    
    def create_vectorstore(self, documents):
        """Cria ou atualiza o vectorstore com os documentos"""
        # Configurar text splitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # Dividir documentos
        splits = text_splitter.split_documents(documents)
        
        # Criar embeddings
        embeddings = OpenAIEmbeddings(
            openai_api_key=self.api_key,
            model="text-embedding-3-small"
        )
        
        # Criar ou atualizar vectorstore
        if self.vectorstore is None:
            self.vectorstore = FAISS.from_documents(splits, embeddings)
        else:
            new_vectorstore = FAISS.from_documents(splits, embeddings)
            self.vectorstore.merge_from(new_vectorstore)
        
        # Salvar vectorstore
        self.vectorstore.save_local(self.vectorstore_path)
        
        return len(splits)
    
    def load_vectorstore(self):
        """Carrega vectorstore existente"""
        try:
            embeddings = OpenAIEmbeddings(
                openai_api_key=self.api_key,
                model="text-embedding-3-small"
            )
            self.vectorstore = FAISS.load_local(
                self.vectorstore_path, 
                embeddings,
                allow_dangerous_deserialization=True
            )
            return True
        except:
            return False
    
    def get_available_models(self):
        """Retorna a lista de modelos dispon√≠veis na API da OpenAI."""
        if not self.client:
            return []
        try:
            return self.client.models.list().data
        except Exception as e:
            st.error(f"Erro ao buscar modelos: {e}")
            return []

    def setup_qa_chain(self, model_name="gpt-4o-mini", temperature=0.3):
        """Configura a chain de perguntas e respostas"""
        if self.vectorstore is None:
            return False
        
        # Configurar o modelo
        llm = ChatOpenAI(
            openai_api_key=self.api_key,
            model_name=model_name,
            temperature=temperature
        )
        
        # Configurar mem√≥ria
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Template de prompt personalizado
        prompt_template = """Voc√™ √© um assistente especializado da empresa. 
        Use o contexto fornecido para responder √† pergunta de forma precisa e profissional.
        Se n√£o souber a resposta com base no contexto, diga claramente que n√£o tem essa informa√ß√£o.
        
        Contexto: {context}
        
        Hist√≥rico da conversa: {chat_history}
        
        Pergunta: {question}
        
        Resposta detalhada e profissional:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        
        # Criar chain
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": PROMPT},
            return_source_documents=True,
            verbose=False
        )
        
        return True
    
    def query(self, question):
        """Realiza uma consulta ao sistema"""
        if self.qa_chain is None:
            return None, []
        
        result = self.qa_chain({"question": question})
        return result["answer"], result["source_documents"]

def main():
    # T√≠tulo principal
    st.markdown('<h1 class="main-header">ü§ñ Sistema RAG Empresarial</h1>', unsafe_allow_html=True)
    
    # Inicializar sistema
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RAGSystem()
    
    rag = st.session_state.rag_system
    
    # Validar API Key
    if not rag.validate_api_key():
        st.stop()
    
    # Sidebar para configura√ß√µes
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # Upload de documentos
        st.subheader("üìÑ Upload de Documentos")
        uploaded_files = st.file_uploader(
            "Selecione arquivos PDF",
            type=['pdf'],
            accept_multiple_files=True,
            help="Fa√ßa upload de documentos PDF para adicionar √† base de conhecimento"
        )
        
        if uploaded_files:
            if st.button("üîÑ Processar Documentos", use_container_width=True):
                with st.spinner("Processando documentos..."):
                    documents = rag.load_documents(uploaded_files)
                    num_chunks = rag.create_vectorstore(documents)
                    st.success(f"‚úÖ {len(documents)} documentos processados em {num_chunks} chunks!")
        
        st.divider()
        
        # Configura√ß√µes do modelo
        st.subheader("üß† Configura√ß√µes do Modelo")
        
        model_name = st.selectbox(
            "Modelo OpenAI",
            ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
            help="Escolha o modelo da OpenAI a ser usado"
        )
        
        temperature = st.slider(
            "Temperatura",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="Controla a criatividade das respostas (0 = mais conservador, 1 = mais criativo)"
        )
        
        if st.button("üöÄ Inicializar Sistema", use_container_width=True):
            with st.spinner("Inicializando..."):
                if rag.load_vectorstore():
                    if rag.setup_qa_chain(model_name, temperature):
                        st.success("‚úÖ Sistema inicializado com sucesso!")
                    else:
                        st.error("‚ùå Erro ao configurar o sistema de QA")
                else:
                    st.warning("‚ö†Ô∏è Nenhuma base de dados encontrada. Fa√ßa upload de documentos primeiro.")
        
        st.divider()
        
        # Informa√ß√µes do sistema
        st.subheader("‚ÑπÔ∏è Status do Sistema")
        
        # Status dos componentes
        col1, col2 = st.columns(2)
        
        with col1:
            if rag.vectorstore:
                st.success("‚úÖ Base carregada")
            else:
                st.info("‚è≥ Base n√£o carregada")
        
        with col2:
            if rag.qa_chain:
                st.success("‚úÖ QA ativo")
            else:
                st.info("‚è≥ QA n√£o inicializado")
        
        # Estat√≠sticas
        if rag.client:
            if st.button("üìä Ver Estat√≠sticas da API", use_container_width=True):
                with st.spinner("Carregando..."):
                    try:
                        models_count = len(rag.get_available_models())
                        st.metric("Modelos Dispon√≠veis", models_count)
                    except:
                        st.info("N√£o foi poss√≠vel carregar estat√≠sticas")
        
        # Limpar hist√≥rico
        if st.button("üóëÔ∏è Limpar Hist√≥rico", use_container_width=True):
            if rag.memory:
                rag.memory.clear()
                st.success("Hist√≥rico limpo!")
                st.session_state.messages = []
    
    # √Årea principal - Chat
    st.header("üí¨ Chat com seus Documentos")
    
    # Inicializar hist√≥rico de chat
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Exibir hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("üìö Fontes"):
                    for i, source in enumerate(message["sources"], 1):
                        st.write(f"**Fonte {i}:** {source.metadata.get('source', 'Desconhecido')} - P√°gina {source.metadata.get('page', 'N/A')}")
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta aqui..."):
        # Adicionar mensagem do usu√°rio
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Gerar resposta
        if rag.qa_chain:
            with st.chat_message("assistant"):
                with st.spinner("Pensando..."):
                    answer, sources = rag.query(prompt)
                    
                    if answer:
                        st.markdown(answer)
                        
                        # Mostrar fontes
                        if sources:
                            with st.expander("üìö Fontes consultadas"):
                                for i, source in enumerate(sources, 1):
                                    st.write(f"**Fonte {i}:** {source.metadata.get('source', 'Desconhecido')} - P√°gina {source.metadata.get('page', 'N/A')}")
                        
                        # Adicionar resposta ao hist√≥rico
                        st.session_state.messages.append({
                            "role": "assistant", 
                            "content": answer,
                            "sources": sources
                        })
                    else:
                        st.error("N√£o foi poss√≠vel gerar uma resposta.")
        else:
            with st.chat_message("assistant"):
                st.warning("‚ö†Ô∏è Por favor, inicialize o sistema primeiro usando o bot√£o na barra lateral.")
    
    # Informa√ß√µes de ajuda
    with st.expander("‚ùì Como usar este sistema"):
        st.markdown("""
        <div class="info-box">
        
        ### üöÄ In√≠cio R√°pido:
        
        1. **Configure sua API Key**: Adicione sua chave da OpenAI no arquivo `.env`
        2. **Carregue Documentos**: Use a barra lateral para fazer upload de PDFs
        3. **Processe os Documentos**: Clique em "Processar Documentos"
        4. **Inicialize o Sistema**: Clique em "Inicializar Sistema"
        5. **Fa√ßa Perguntas**: Digite suas perguntas no chat
        
        ### üí° Dicas:
        
        - O sistema mant√©m o contexto da conversa
        - Voc√™ pode fazer perguntas de acompanhamento
        - As fontes s√£o sempre citadas para transpar√™ncia
        - Ajuste a temperatura para controlar a criatividade das respostas
        
        ### üéØ Casos de Uso:
        
        - **Advocacia**: Consulta r√°pida a contratos e jurisprud√™ncias
        - **Desenvolvimento**: Acesso a documenta√ß√£o t√©cnica e procedimentos
        - **RH**: Informa√ß√µes sobre pol√≠ticas e benef√≠cios
        - **Financeiro**: An√°lise de relat√≥rios e demonstrativos
        
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()